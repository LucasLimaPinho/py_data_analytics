# Python é uma linguagem interpretada - o que o faz rodar de forma mais lenta
# do que um código de linguagem compilada como C++ ou Java.

# DataFrame é orientado a colunas, estrutura de dados tabular, com labels
# tanto para linhas quanto para colunas
# Series -> objeto array UNIDIMENSIONAL com rótulo
# Para dados numéricos, os arrays NumPy são mais eficientes para armazenar 
# e manipular dados do que as outras estruturas de dados embutidas (built-in)
# Além do mais, as bibliotecas escritas em uma linguagem de baixo nível, como
# C ou Fortran, podem operar em dados armazenados em um arram NumPy sem 
# copiar dados para outra representação em memória.

# Pandas combina as ideias de processamento de alto desempenho de arrays da NumPy
# com os recursos flexíveis de manipulação de dados das planilhas e dos
# banco de dados relacionais (como o SQL). Ele disnponibiliza uma funcionalidade
# sofisticada de indexação para facilitar a reformatação, a manipulação,
# as agregações e a seleção de subconjunto de dados.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy
import sklearn
import statsmodels as sm
import seaborn as sns
import timeit
%clear

# Lista b -> b.append, b. clear, b.copy, b.count, b.insert,
# b.pop, b.remove, b.reverse, b.sort

# Módulo datetime -->> datetime.date, datetime.datetime, datetime.datetime_CAPI
# datetime.MAXYEAR, datetime.MINYEAR, datetime.time, datetime.timedelta,
# datetime.timezone, datetime.tzinfo

b = [1,2,3]
b?
%clear
def add_numbers(a,b):
    return a + b
add_numbers?
add_numbers??

a = [1,2,3]
b = a
a
b
a.append(4)   
a
b 
# OBSERVAR que em Python agora 'b = a' se referem ao mesmo objeto
# e não somente b recebeu os valores [1,2,3]. A atribuição também é chamada
# de vinculação (binding), pois estamos vinculando um nome a um objeto.
# Quando passamos objetos como argumentos para uma função, novas variáveis
# locais são criadas para referenciar os objetos originais, sem qualquer cópia.
# Se você vincular um novo objeto a uma variável em uma função,
# essa alteração não se refletirá no escopo-pai. ASsim é possível alterar
# o conteúdo interno de um argumento mutável.

# Objetos em Python geralmente têm tantos atributos (outros objetos Python
# "armazenados" dentro do objeto) quando métodos (funções associadas a um objeto
# que podem ter acesso a seus dados internos). Ambos são acessados 
# por meio da sintaxe obj.nome_do_atributo

# Atributos e métodos também podem ser acessados pelo nome usando a função
# getattr

##### DUCK TYPING: "Se anda como pato e faz quack como um pato, então é um pato"

def isiterable(obj):
    try:
        iter(obj)
        return True
    except TypeError:
        return False

isiterable('a string')
isiterable([1,2,3])
isiterable(5)

if not isintance(x,list) and isiterable(x):
    x = list(x)

a = [1,2,3]
c = list(a) # list sempre cria uma nova lista Python (cópia)
a is c
a == c
d = """this is a multiple
string that spans
multiple 
lines"""
print(d)
d.count('\n')
a = 'this is a string'
a[10]
for iter in a:
    print(iter)
for iter in enumerate(a):
    print(iter)
x = 5.6
y = str(x)

a = 'this is the first half'
b = ' and this is the second half'
print(a+b)
a + b
# Os objetos string têm um método FORMAT que pode ser usado para
# substituir argumentos formatados na string, gerando uma nova string:

template = '{0:.2f} {1:s} are worth US${2:d}'

# {0:.2f} significa formatar o primeiro argumento como um número de ponto
# flutuante com duas casas decimais
# {1:s} significa formatar o segundo argumento como uma string
# {2:d} significa formatar o terceiro argumento como um inteiro exato

template.format(4.5560,'Argentine Pesos',1)

# Converter string Unicode para sua representação em bytes UTF-8 usando
# o método encode

val = 'español'
val
val_utf8 = val.encode('utf-8')
val_utf8
type(val_utf8)
val_utf8.decode('utf-8')
val.encode('latin1')

# Casting de tipos

# Os tipos str, bool, int e float também são funções que podem ser usadas 
# para cast de valores para esses tipos

s = '3.14159'
type(s)
fval = float(s)
fval
type(fval)
int(fval)
bool(fval)
bool(0)

# None pode ser usado como valor default para argumento de função

def add_and_maybe_multiply(a,b,c=None):
    result= a+b
    if c is not None:
        result = result * c
    return result
add_and_maybe_multiply(3,5)
add_and_maybe_multiply(3,5,3)

### Datas e Horas

# O módulo built-on DATETIME de Python disponiibliza os tipos 
# datetime, date e time
# O tipo datetime, como você pode imaginar, combina as informações armazenadas
# em date e time e é o tipo mais comumente utilizado:

from datetime import datetime, date, time
dt = datetime(2011,10,29,20,30,21)
dt.day
dt.minute
dt.hour
dt.second
dt.date()
# Dada uma instância de datetime, você pode extrair os objetos date e time
# equivalentes chamando os métodos no datetime

dt.time()
dt.strftime('%m/%d/%Y %H:%M')
dt.strptime('20091031', '%Y%m%d')
dt.replace(minute=0,second=0)
dt2 = datetime(2011,11,15,22,30)
delta = dt2 - dt
delta
type(delta)
dt + delta

string = 'Squeezing Data 4 a living'
del seq
for i in range(len(string)):
    print(i)

# Tuplas: sequências imutáveis de tamanho fixo.
tup = 4,5,6
type(tup)
tup
nested_tup = (4,5,6), (7,8)
nested_tup
len(nested_tup)
tup = tuple('string')
len(tup)
tup
tup[0]
# Não podemos alterar o valor das tuplas -> coleções imutáveis
tup2 = tuple(['foo',[1,2],True])
tup2[2] = False # Gera erro por ser uma coleção imutável
# Se um objeto de uma tupla for mutável, por exemplo, uma lista, você
# pode alterá-lo in place.
tup2[1] # -> é a lista [1,2] que é uma coleção mutável.
tup2[1].append(3)
tup2
#Concatenação de tuplas utilizando operador +
# Multiplicar uma tupla por um inteiro, como ocorre com as listas,
# tem o efeito de concatenar essa quantidade de cópias da tupla:
tup3 = (4, None, 'foo') + (6,0) + ('bar',)
tup3
tup3*3
('foo','bar')*4
# DESEMPACOTANDO TUPLAS

tup = (4,5,6)
a,b,c, = tup
a
b
c
tup = 4,5,(6,7)
a,b,(c,d) = tup
d
# usando desempacotamento para swap;

values = 1,2,3,4,5
a,b,*resto = values
a,b
resto
# AS vezes por convenção utilizamos o underscore quando não usaremos o resto

a,b,*_ = values
a,b
_
b_list = list(values)
b_list
type(b_list)
b_list[1] = 'peekaboo' # lista é mutável, agoro posso realizar alteração.
b_list
b_list.append('dwarf');b_list
b_list.insert(1,'red');b_list
b_list.insert(3,'posição3');b_list
# insert é custosa do ponto de vista de processamento se comparada com append.
# collections.deque pode ser melhor para inserir ou retirar elementos do começo
# e fim de listas.

b_list.pop(2)
b_list
b_list.append('foo');b_list
b_list.remove('foo');b_list
'dwarf' in b_list
'dwarf' not in b_list
# Verfificar se uma lista contém um valor é muito mais lendo do que fazer 
# isso com dicionários e conjuntos, pois Python faz uma verificação linear
# nos valores de lista, enquanto é capaz de verificar os outros tipos (com 
#base em tabelas hash) em um tempo constante.

b_list.extend(['banco','fintech','dot','instituto']);b_list
# Usar .extend é preferível em relação a concatenar com operador +
b_list[0] = str(b_list[0]);b_list[3] = str(b_list[3]); b_list[4] = str(b_list[4]); b_list[5] = str(b_list[5])

b_list.sort(key=len);b_list
import bisect
# bisect implementa a busca binária e inserção em uma lista ordenada. 
# bisect.bisect encontra o local em que um elemento deve ser inserido para manter a lista 
# ordenada, enquanto bisect.insort insere o elemento nesse local.

c = [1,2,2,2,3,4,7]
bisect.bisect(c,2)
bisect.bisect(c,5)
bisect.insort(c,6)
c
# SLICING
seq = [7,2,3,7,5,6,0,1]
seq[1:5]
seq[3:4] = [6,3]
seq[:5]
seq[3:]
seq[-4:]
seq[-6:-2]
seq[::2]
seq[::-1] # Inverte uma lista ou uma tupla!

# ENUMERATE: querer manter o controle do índice do item atual quando iteramos
# por uma sequência é comum. Enumerate devolve uma sequência de tuplas.

alguma_lista = ['foo','bar','baz']
mapping = {}
for i,v in enumerate(alguma_lista):
    mapping[v]=i
mapping

# ZIP pareia os elementos de uma série de listas, tuplas ou outras sequências para
# criar uma lista de tuplas:

seq1 = ['foo','bar','baz']; seq2 = ['one','two','three']
zipped = zip(seq1,seq2)
zipped
list(zipped)
# zip pode aceitar um número arbitrário de sequências, e o número de elementos que ele gera é 
# determinado pela sequência mais curta.

seq3 = [True,False]
zipped = zip(seq1,seq2,seq3)
list(zipped)

for i,(a,b) in enumerate(zip(seq1,seq2)):
    print('{0}: {1}, {2}'.format(i,a,b))


# dict provavelmente é a estrutura de dados embutida mais importante de Python.
# um nome mais comum para ele é hash map ou array associativo. 
# consiste em uma coleção de pares de chave-valor (KVP) de tamanho flexível
# em que chave e valor são objetos Python. Lidando com dicionário, argumento deve ser
# tipicamente, a chave.
empty_dict = {}
d1 = {'a': 'some value', 'b': [1,2,3,4]}
d1['b']
d1['a']
d1[7] = 'an integer'
d1
'b' in d1 # True
b in d1 # False

d1[5] = 'some value';d1
d1['dummy'] = 'another value'; d1
del d1[5]; d1
ret = d1.pop('dummy'); ret # pop retorna o value e recebe a key para apagar.
d1.keys()
list(d1.keys())
list(d1.values())
d1.update({'b': 'foo', 'c': 12})
d1

# value = some_dict.get(key,default_value) -> by default, get vai devolver None se a chave não
# estiver presente. some_dict.pop() irá lançar uma exceção caso a chave não esteja presente.

words = ['apple','bat','bar','atom','book']
by_letter = {}
for word in words:
    letter = word[0]
    if letter not in by_letter:
        by_letter[letter] = [word]
    else:
        by_letter[letter].append(word)

# O método de dicionário setdefault serve exatamente para essa finalidade. 
# o laço for anterior pode ser reescrito assim:

for word in words:
    letter = word[0]
    by_letter.setdefault(letter,[]).append(word)

%reset
%clear
# os OBJETOS HASHABLE são imutáveis. Portanto, listas que são mutáveis são unhashable. 
hash('string) #OK porque string é imutável - hashable
hash((1,2,(2,3))) #OK porque tupla é imutável - hashable
hash((1,2,[2,3])) #NOK -> listas são mutáveis e, portanto, não hashable.

# LIST COMPREHENSIONS (listcomps): permitem que você componha uma nova lista de nmodo conciso, filtrando os elementos de uma coleção,
# transformando os elementos aos passar o filtro, com uma expressão concisa



# forma padrão de listcomp ---> [expr for val in collection if condition]

strings = ['a', 'as', 'bat', 'car', 'dove', 'python']
listcomp = [x.upper() for x in strings if len(x) > 2]


# dict_comp = {expr-chave : expr-valor for value in collection if condition}
# set_comp = {expr for value in collection if condition}

loc_mapping = {val : index for index, val in enumerate(strings)}
loc_mapping

# funções em python são objetos. Facilita remover espaços em branco, símbolos de pontuação, criar um padrão para ter um uso adequado de 
# letras maiúsculas ou não no início das palavras. Uma forma de fazer isso é usar métodos embutidos de string, junto com o módulo re
# da bibioleca-padrão para expressões regulares

import re
states = ['Alab?ama',
          'Georgia!',
          'georgia',
          'FlOrida',
          'south carolina####',
          'West Virginia?']
def clean_strings(strings):
    result = []
    for value in strings:
        value = value.strip()
        value = re.sub('[!#?]','',value)        
        value = value.replace(" ","")
        value = value.title()
        result.append(value)
    return result

clean_strings(states)

# FUNÇÕES ANÔNIMAS (lambdas): Python oferece suporte para funções anônimas ou lambdas - uma forma de escrever funções constituídas 
# de uma única instrução, cujo resultado é o valor de retorno. Lambda significa "estamos declarando uma função anônima"
# Em Data Science, há muitos casos em que funções de transformação de dados aceitarão funções como argumentos -
# daí a necessidade de termos que usar funções lambdas. Geralmente exige-se menos digitação (e é mais claro) passar
# uma função lambda, em oposição a escrever uma declaração de função completa ou até mesmo abrir a função lambda a uma variável local.

 # CURRYING: aplicação parcial de argumentos derivando novas funções a partir de funções existentes.
 
 def add_numbers(x,y):
                return x + y
 add_five = lambda y: add_numbers(5,y) # função de uma variável derivada da função existente -> currying.
 # Dizemos que o segundo argumento de add_numbers sofreu currying.
 
 from functools import partial
 add_five = partial(add_numbers,5)
 # EXPRESSÕES GERADORAS: feitas utilizando parênteses ao invés de colchetes na sintaxe de listcomps.
 # Geradores e expressões geradores atuam de forma lazy e somente geram quando chamados.
 # Expressões geradores podem ser usadas no lugar de listcomps como argumentos de função em muitos casos.
 
 gen = (x ** 2 for x in range(100))
 
 ## Módulo ITERTOOLS
 
 # O módulo itertools da biblioteca-padrão tem uma coleção de geradores para muitos algoritmos comuns de dados.
 # Por exemplo, groupby aceita qualquer sequência e uma função, agrupando elementos consecutivos da sequeência de acordo
 # com o valor de retorno da função.
 
 import itertools
 first_letter = lambda x: x[0]
 names = ['Alan', 'Adam', 'Wes', 'Will', 'Albert', 'Steven']
 for letter, names in itertools.groupby(names,first_letter):
        print(letter,list(names)) # names é um gerador
   
######## NUMPY: arrays e processamento vetorial.]
# 1) ndarray: um array multidimensional eficaz que oferece operações aritméticas
     #rápidas, orientadas a arrays, e recursos flexíveis de broadcasting
# 2) funções matemáticas para operações rápidas de arrays de dados inteiros, 
     # sem que seja necessário escrever laços
# 3) ferramentas para ler/escrever dados de array em disco e trabalhar com 
     # arquivos mapeados em memória
# 4) recursos para álgebra linear, geração de números aleatórios e transformadas
     # de Fourier
# 5) uma API C para conectar o NumPy a bibliotecas escritas em C, C++ ou FORTRAN
# esse recurso fez de Python uma linguagem preferida para encapsular bases de código
# legadas em C/C++/FORTRAN, oferecendo-lhes uma interface dinâmica e fácil
# broadcasting é um dos recursos mais avançados de Numerical Python (NumPy)     
%reset
%clear
import numpy as np
     # NumPy oferece operações rápidas em arrays vetorizados para tratamento 
# e limpeza de dados, geração de subconjuntos e filtragem, transformações e
# outros tipos de processamentos.]

# algoritmos comuns para arrays como ordenação, 
#unicidade e operações de conjunto

# estatísticas descritivas eficazes e agregação/sintetização de dados;

# alinhamento de dados e manipulações de dados relacionais
# para combinar e juntar conjuntos de dados heterogêneos;

# expressão de lógica condicionak na forma de expressões de array
# em vez de laços com ramos if-else-elif]

# manipulações de dados em grupos (agregação, transformação e aplicação de função)

# pandas é mais utilizado em dados tabulares. O pandas também oferece algumas funcionalidades mais específicas de domínios,
# como manipulação de séries temporais, que não estão presentes no NumPy.

######## NUMPY: arrays e processamento vetorial.]
# 1) ndarray: um array multidimensional eficaz que oferece operações aritméticas
     #rápidas, orientadas a arrays, e recursos flexíveis de broadcasting
# 2) funções matemáticas para operações rápidas de arrays de dados inteiros, 
     # sem que seja necessário escrever laços
# 3) ferramentas para ler/escrever dados de array em disco e trabalhar com 
     # arquivos mapeados em memória
# 4) recursos para álgebra linear, geração de números aleatórios e transformadas
     # de Fourier
# 5) uma API C para conectar o NumPy a bibliotecas escritas em C, C++ ou FORTRAN
# esse recurso fez de Python uma linguagem preferida para encapsular bases de código
# legadas em C/C++/FORTRAN, oferecendo-lhes uma interface dinâmica e fácil
# broadcasting é um dos recursos mais avançados de Numerical Python (NumPy)     
%reset
%clear
import numpy as np
     # NumPy oferece operações rápidas em arrays vetorizados para tratamento 
# e limpeza de dados, geração de subconjuntos e filtragem, transformações e
# outros tipos de processamentos.]

# algoritmos comuns para arrays como ordenação, 
#unicidade e operações de conjunto

# estatísticas descritivas eficazes e agregação/sintetização de dados;

# alinhamento de dados e manipulações de dados relacionais
# para combinar e juntar conjuntos de dados heterogêneos;

# expressão de lógica condicionak na forma de expressões de array
# em vez de laços com ramos if-else-elif]

# manipulações de dados em grupos (agregação, transformação e aplicação de função)

# Comparação de desempenho de um array NumPy com 10^6 milhão de inteiros
# e uma lista equivalente em Python

my_arr = np.arange(10**6)
my_list = list(range(10**6))
%time for _ in range (10): my_arr2 = my_arr*2 
%time for _ in range (10): my_list2 = [x*2 for x in my_list]

# Algoritmos baseados no NumPy geralmente são de 10 a 100 vezes mais rápidos (ou mais) do que suas contrapartidas em Python puro,
# além de utilizarem significativamente menos memória.

# NDARRAY no NumPy: um objeto array multidimensional

# contêiner rápido e flexível para conjuntos de dados grandes em Python. Arrays permitem realizar operações matemáticas
# em blocos inteiros de dados usando uma sintaxe semelhante às operações equivalentes entre elementos escalares.

# um ndarray é um contêiner genérico multidimensional para dados homogêneos; isso significa que todos os elementos
# DEVEM SER DO MESMO TIPO. Todo array tem um SHAPE, isto é, uma tipla que indica o tamanho de cada dimensão, e um
# DTYPE, que é um objeto que descreve o tipo de dado do array.
# SHAPE e DTYPE -> coisa de array; Para ter shape precisa ser multidimensional. Dict não tem shape.

data.shape
data.dtype

my_arr = np.arange(10**6)
my_list = list(range(10**6))
%time for _ in range (10): my_arr2 = my_arr*2 
%time for _ in range (10): my_list2 = [x*2 for x in my_list]   

data = np.random.randn(2,3) # dados com distribuição normal. 2 linhas, 3 colunas
data
data *10
data + data     
data.shape
data.dtype

data1 = [6,7.5,8,0,1]
arr1 = np.array(data1)
arr1

data2 = [[1,2,3,4],[5,6,7,8]] # vírgula separa a linha na formação do array
len(data2)
type(data2)
arr2 = np.array(data2)
type(arr2)
len(arr2)
arr2.size
arr2.shape
arr2.dtype
arr2.ndim # importante para verificar quantidade de atributos em data science
arr2
np.zeros(10)
np.ones(5)
np.zeros((3,5))
np.empty((2,3,2))
np.eye(3)
np.full(5,3)

# Os dtyés são um dos motivos para a flexibilidade do NumPy em interagir com dados provenientes de outros sistemas. 
# Na maioria dos casos, eles ofrecem um mapeamento direto para um disco subjacente ou uma representação em memória,
# o que facilita ler e escrever streams de dados binários em disco e também se conectar com um código escrito
# em linguagem de baixo nível como C ou Fortran. 

# Cast np.array
arr = np.array([1,2,3,4,5])
arr.dtype
float_arr = arr.astype(np.float64)
float_arr.dtype
float_arr


arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])
arr
int_arr = arr.astype(np.int32)
int_arr
# Se, por algum motivo, o casting falhar (como no caso de uma string que não possa ser convertida para float64),
# um ValueError será gerado.
# Chamar astype sempre cria um novo array (uma cópia de dados), mesmo que o novo dtype seja igual ao dtype antigo.
# Aritméticas com arrays NumPy: permitem expressar operações em lote nos dados, sem escrever qualquer laço for.

import numpy as np
arr = np.array([[1.,2.,3.],
               [4.,5.,6.]]); arr.dtype
arr.shape
arr
arr * arr
arr - arr
1 / arr
arr ** 0.5

arr2 = np.array([[0.,4.,1.],
                 [7.,2.,12.]]); arr2

arr2 > arr

# As operações entre arrays de tamanhos distintos são chamados de BROADCASTING.
# Atribuir valor escalar a uma fatia, o valor será propagado (sofrerá broadcast a partir daí) para toda a seleção.
# As fatias de arrays são VISUALIZAÇÕES do array original. Issto significa que os dados não são copiados e qualquer modificação
# na visualização se refletirá no array original.

arr = np.arange(10); arr
arr[5]
arr[5:8]
arr[5:8] = 12; arr
arr_slice = arr[5:8]; arr_slice
arr_slice[1] = 12345; arr
# Quando faço alterações nos valores de arr_slice, as mudanças se refletem no array arr original.
arr_slice[:] = 64; arr
# Poderíamos ter sérios problemas de desempenho e de memória se o NumPy insistisse em sempre copiar dados, uma vez que
# iremos lidar com problemas de dados bem grandes.

# Se você quiser uma cópia de uma fatira de um ndarray em vez de ter uma visualização, será necessároi copiar explicitamente o array -
# por exemplo arr[5:8].copy()

arr = np.arange(10); arr
arr[5]
arr[5:8]
arr[5:8] = 12; arr
arr_slice = arr[5:8]; arr_slice
arr_slice[1] = 12345; arr
arr_slice[:] = 64; arr

arr2d = np.array([[1,2,3],
                  [4,5,6],
                  [7,8,9]]); arr2d[2]
arr2d[0,2]

arr3d = np.array([[[1,2,3],
                  [4,5,6]],
                  [[7,8,9],
                  [10,11,12]]]); arr3d
arr3d.shape
arr3d[0]
arr3d[1]
arr3d[0].shape
arr3d[1].shape
arr3d[0,0,2]
arr3d[0][0][2]
old_values = arr3d[0].copy()
arr3d[0] = 42
arr3d
arr3d[0] = old_values; arr3d

# Indexando com fatias

arr3d.shape
arr3d[0]
arr3d[1]
arr3d[0].shape
arr3d[1].shape
arr3d[0,0,2]
arr3d[0][0][2]
old_values = arr3d[0].copy()
arr3d[0] = 42
arr3d
arr3d[0] = old_values; arr3d
arr3d[1,0]
arr3d
arr3d[0,:1,:2]

# Indexação booleana

# Assim como nas operações aritméticas, as comparações (como ==) com arrays também são vetorizadas. 

names = np.array(['Bob','Joe','Will','Bob','Will','Joe','Joe'])
names.shape
data = np.random.randn(7,4); data.shape
data
names
names == 'Bob'
type(names=='Bob')
data[names=='Bob'] # Devolve os índices em que names == 'Bob'
# em ndarray do NumPy, os itens passados como argumentos se referem às linhas

# O array booleano deve ter o mesmo tamanho do eixo do array que ele está indexado. Você pode até mesmo misturar e fazer a
# correspondência entre arrays booleanos e fatias ou inteiros (ou sequência de inteiros)

# em ndarray do NumPy, os itens passados como argumentos se referem às linhas
data[names=='Bob',2:]
data[names=='Bob',3]
data[~(names=='Bob')] # índices em que data não é 'Bob'. No caso, 7 -2 =5
data[~(names=='Bob')].shape
cond = names == 'Bob'
data[~cond]
mask = (names=='Bob') | (names == 'Will')
mask
data[mask]
# As palavras reservadas and e or de Python não funcionam com arrays booleanos. Utilize & (and) | (or) em seu lugar.
data[data<0] = 0; data
data[names != 'Joe'] = 7; data

# Indexação Sofisticada (fancy indexing)

# Termo adotado pelo NumPy para descrever a indexação usando arrays de inteiros. Suponha que tivéssemos um array de 8x4.

arr = np.empty((8,4))
arr = np.empty((8,4))
for i in range(8):
    arr[i] = i
arr
arr[[4,3,0,6]]
arr[[-3,-5,-7]] # Usar índices negativos seleciona as linhas a partir do final (a indexação começa em -1 e não em 0).

# Passar vários índices de array faz algo um pouco diferente; a instrução seleciona um array unidimensional de elementos
# correspondentes a cada tupla de índices:


arr = np.arange(32).reshape((8,4))
arr
arr = np.arange(32).reshape((4,8)); arr
arr = np.arange(32).reshape((8,4)); arr
arr[[1,5,7,2], [0,3,1,2]] # Nesse caspo, os elementos (1,0), (5,3), (7,1), (2,2) foram selecionados. Independente
# de quantas dimensoes o array tiver, o resultado da indexação sofisticada sempre será unidimensional.
# A indexação sofisticada, de modo diferente do slicing, sempre copia os dados para um novo array

arr[[1,5,7,2][:,[0,3,1,2]]

# Transposição de arrays e troca de eixos.

# A transposição devolve uma visualização dos dados subjacentes, sem copiar nada. Os arrays possuem o método transpose.

arr = np.array([[0,1,2,3,4],
       [5,6,7,8,9],
       [10,11,12,13,14]])
arr.T
arr.T.shape
arr = np.random.randn(6,3)
np.dot(arr.T,arr)

# np.dot é o PRODUTO INTERNO (INNER PRODUCT)

arr = np.arange(16).reshape(2,2,4)
arr
arr.transpose(1,0,2)
arr
arr.swapaxes(1,2)

arr = np.random.randn(7)*5
remainder , whole_part = np.modf(arr)
remainder
whole_part

# UFUNCs permitem utilizar argumento opcional out que lhes permite atuar in-place nos arrays.

# Pg.146 traz uma lista de ufuncs aplicadas a arrays como abs, fabs, sqrt, square, exp, log, log10, log2
# log1p, sign, floor, ceil, rint, modf, isnan, isfinite, isinf, cos, sin, cosh, sinh, tan, tanh,

# arccos, arccosh, arcsin, arcsinh, arctan, arctanh,
#logical_not

# Programação Orientada a arrays: evitar escrita de laços; concise expressions;
# a prática de substituição de laços explícitos por expressões de arrays é chamada de vetorização (vectorization).
# é mais rápido de uma a duas ordens de grandeza que os equivalentes em Python puro.

points = np.arange(-5,5,0.01) # 1000 pontos igualmente espaçados
xs, ys = np.meshgrid(points,points)
ys
xs

import matplotlib.pyplot as plt
z = np.sqrt(xs**2 + ys **2)
plt.imshow(z,cmap=plt.cm.gray);plt.colorbar()
plt.title("Image plot of $\sqrt {x^2 + y^2}$ for a grid of values")

# numpy.where é uma função ternária - versão vetorizada x if condition else y. 

xarr = np.array([1.1,1.2,1.3,1.4,1.5])
yarr = np.array([2.1,2.2,2.3,2.4,2.5])
cond = np.array([True,False,True,True,False])

# Versão com laço explícito x if conditio else y

result = [(x if c else y) for x,y,c in zip(xarr,yarr,cond)]
result

# Versão vetorizada com função ternária numpy.where

result = np.where(cond, xarr, yarr)
result
# O segundo e o terceiro argumento de np.where não precisam necessariamente 
# ser arrays; um deles, ou ambos, podem ser escalares.

# O segundo e o terceiro argumento de np.where não precisam necessariamente 
# ser arrays; um deles, ou ambos, podem ser escalares. Um uso típico de where em análise de dados
# é aquele em que geramos um novo array de valores com base em outro array.
# Suponha que você tenha uma matriz de dados gerados aleatoriamente e quisesse
# substituir todos os valores positivos por 2 e todos os negativos por -2.
# Fica fácil utilizando numpy.where.

arr = np.random.randn(4,4)
np.where(arr >0,2,-2)

arr = np.random.randn(5,4)
arr
arr.mean()
np.mean(arr)
arr.sum()
arr.mean(axis=1)
arr.sum(axis=0)
arr = no.array([0,1,2,3,4,5,6,7])
arr.cumsum()

arr = np.random.randn(100)
(arr>0).sum() # numero de valores positivos (deve ser próximo de 50)

# Há dois métodos adicionais, any e all, particularmente úteis para arrays booleanos.
# any testa se um ou mais valores em um array são True, enquanto all verifica
# se todos os valores são True.

bools = np.array([False,False,True,False])
bools.any()
bools.all()

# Podemos ordenar cada seção unidimensional de valores em um array
# multidimensional in place ao longo de um eixo passando o número desse eixo para sort

arr = np.random.randn(5,3)
arr
arr.sort(1)
arr
# NP.UNIQUE

names = np.array(['Bob','Joe','Will','Bob','Will','Joe','Joe'])
np.unique(names)
ints = np.array([3,3,3,2,2,1,1,4,4])
np.unique(ints)

# np.in1d -> testa a pertinência dos valores de um array em outro devolvendo 
# um array booleano

values = np.array([6,00,3,2,5,6])
np.in1d(values,[2,3,6])

# De modo diferente de algumas linguagens como MATLAB, multiplicar dois arrays bidimensionaus com *
# corresponde ao produto de todos os elementos e não a um produto escalar de matrizes. Desse modo, há uma função
# dot, que é tanto um método de array quanto uma função no namespace numpy para multiplicação de matrizes:


x = np.array([[1.,2.,3.], [4.,5.,6.]])
y = np.array([[6.,23.], [-1,7],[8,9]])
x
y
x.dot(y)
np.dot(x,y)

# Um produto de matrizes entre um array bidimensional e um array unidimensional de tamanho apropriado resulta em um array unidimensional:

np.dot(x,np.ones(3))

# O símbolo @ (conforme usado em Python 3.5) também funciona como operador infixo que efetua multiplicação de matrizes:

x @ np.ones(3)

# numpy.linalg tem um conjunto padrão de decomposições de matrizes além de operações como inverso e determinante. Elas estão implementadas
# internamente por meio das mesmas bibloptecas de álgebra linear que são padrões de mercado em outras linguagens como R e MATLAB.
# BLAS, LAPACK.

from numpy.linalg import inv, qr

X = np.random.randn(5,5)
mat = X.T.dot(X)
inv(mat)
mat.dot((inv(mat)))
q, r = qr(mat)

# PG. 159 ------> Funções de LINALG utilizadas e posteriormente um pouco sobre random walks


############# INTRODUCAO AO PANDAS

import pandas as pd
from pandas import Series, DataFrame

# A principal diferença para o NumPy é que o Pandas foi pensado para trabalhar com dados tabulares e heterogêneos.
# O NumPy, em comparação, é mais apropriado para trabalhar com dados numéricos homogêneos em arrays

# Duas estruturas de dados são a força de trabalho do Pandas: Series e DataFrame

# SERIES: array unidimensional contendo uma sequência de valores (de tipos semelhantes aos tipos de NumPy)
# e um array associado de rótulos (labels) chamado de índice. A Series mais simples é composta de
# apenas um array de dados:

obj = pd.Series([4,7,-5,3])

# Com frequência será desejável criar uma Series com um índice que identifique cada ponto de dado com um rótulo:

obj2 = pd.Series([4,7,-5,3], index = ['d', 'b','a','c'])
obj2
obj2.values
obj2.index
obj2['a']
obj2['d'] = 6
obj2[['c','a','d']]
obj2[obj2>0]
obj2*2
np.exp(obj2)
# Podemos pensar Series como um dicionário ordenado de tamanho fixo como se fosse um mapeamento entre valores de índices
# e valores de dados. Ela pode ser utilizada em muitos contextos em que um dicionário poderia ser usado:

'b' in obj2
'e' in obj2
# Se você tiver dados contidos em um dicionário Python, uma Series poderá ser Criada a partir dele, passando-lhe o dicionário:

sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
obj3 = pd.Series(sdata)
type(sdata)
type(obj3)
obj3

# É possível mudar as ordens dos índices passando o argumento index. California irá receber NaN pois não tem key no dicionário

states = ['California', 'Ohio', 'Oregon', 'Texas']
obj4 = pd.Series(sdata, index = states)
pd.isnull(obj4)

obj3 + obj4 # Semelhante ao join -> recurso de alinhamento de dados

### DATAFRAME: representa uma tabela de dados retangular e contém uma coleção ordenada de colunas, em que cada uma pode ter 
# um tipo de valor diferente (numérico, string, booleano, etc.). O dataframe tem índice tanto para linha quanto para coluna;
# pode ser imaginado como um dicionário de Series, todos compartilhando o mesmo índice


# Internamente, os dados são armazenados como um ou mais blocos bidimensionais em vez de serem armazenados como uma lista,
# um dicionário ou outra coleção de arrays unidimensionaus. Indexação hierárquica é um dos tópicos mais quentes em DataFrame
# e permite que vá além da representação de array bidimensional.

data = {'state' : ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year' : [2000,2001,2002,2001,2002,2003],
        'pop': [1.5,1.7,3.6,2.4,2.9,3.2]}
frame = pd.DataFrame(data)

type(frame)
frame
pd.DataFrame(data,columns=['year','state','pop'])
frame.head()
frame2 = pd.DataFrame(data,
                      columns = ['year','state','pop','debt'],
                      index = ['one','two','three','four','five','six'])
frame2['state']
type(frame2['state']) # uma coluna de um dataframe pode ser obtida no formato de uma Series.
frame2.year
type(frame2.year) # uma columa de um DF pode ser obtida na forma de um atributo

# As linhas também podem ser obtidas com base na posição ou no nome, com o atributo especial LOC:

frame2.loc['three']
type(frame2.loc['three']) # Series
# As colunas podem ser modificadas por atribuição. Por exemplo, a coluna vazia 'debt' poderia receber um valor escalar ou um
# array de valores.

frame2['debt'] = 16.5
frame2['eastern'] = frame2.state == 'Ohio';frame2
del frame2['eastern']

# Se o dicionário aninhado for passado para o DataFrame, o pandas interpretará as chaves do dicionário mais externo como as colunas
# e as chaves mais internas como os índices das linhas:

# Os objetos Index são imutáveis e, desse modo, não podem ser modificados pelo usuário: a imutabilidade faz com que seja mais seguro
# compartilhar objetos Index entre estruturas de dados.
# Pandas pode possuir INDEX DUPLICADO. 

frame = pd.DataFrame(np.arange(9).reshape((3,3)),
                     index = ['a','c','d'],
                     columns = ['Ohio','Texas','California'])

# SELEÇÃO COM LOC E ILOC

# Para indexação nas linhas do DF com rótulos, os operadores especiais de indexação são LOC e ILOC. Eles permitem selecionar
# um subconjunto de linhas e colunas de um DataFrame com uma notação semelhante àquela do NumPy usando rótulos de eixo (loc) e
# ou inteiros (iloc).

import pandas as pd
import numpy as np

data = pd.DataFrame(np.arange(16).reshape((4,4)),
                    index = ['Ohio','Colorado','Utah','New York'],
                    columns = ['one','two','three','four'])
data
data['two']
data < 5
data.loc['Colorado',['two','three']]
data
data.iloc[2,[3,0,1]]
data.iloc[2]
data.iloc[[1,2],[3,0,1]]
data.loc[:'Utah','two']
data.iloc[:,:3][data.three>5] # condição jogada entre colchetes após segmentação usando iloc

import pandas_datareader.data as web

all_data = {ticker: web.get_data_yahoo(ticker) for ticker in ['AAPL','IBM','MSFT','GOOG']}
price = pd.DataFrame({ticker : data['Adj Close'] for ticker, data in all_data.items()})
volume = pd.DataFrame({ticker : data['Volume'] for ticker, data in all_data.items()})

returns = price.pct_change()
returns.tail()
returns['MSFT'].corr(returns['IBM'])
returns['MSFT'].cov(returns['IBM'])
returns.MSFT.corr(returns.IBM)
returns.corr() # matri completa de correlação 
returns.cov() # matri completa de covariância
returns.corrwith(returns.IBM)

# Entrada e Saída de Dados usando Pandas.
# A entrada e saída de dados geralmente se enquadram em algumas categorias principais: a leitura de arquivos-texto e outros
# formatos mais eficientes em disco, carga de dados de banco de dados e interação com fontes de dados da rede, como APIS web

# 6.1 - Lendo e escrevendo dados em formato - texto (pg. 218)

# read_csv, read_table, read_fwf_ read_clipboard_ read_excel, read_hdf, read_html, read_json
# read_msgpack, read_picle, read_sas, read_sql, read_stata, read_feather

# Converter DADOS DE TEXTO EM UM DATA FRAME
# Parsing de data e hora: inclui recursos de combinação, entre eles, combinação de informações de data e hora
# espalahadas em várias colunas ou em uma única coluna resultado
# Iteração: suport para iteração em partes de arquivos bem grandes
# Problemas com dados sujos: pular linhas ou um rodapé, comentários ou outras pequenas informações como dados numéricos com vírgulas
# para separar milhares

# READ_CSV TEM MAIS DE 50 PARÂMETROS
# algumas funções fazem inferência de tipos, pois os tipos de dados das colunas não fazem parte do formato.
# Isso significa que você não precisa necessariamente especificar quais colunas são numéricas, inteiras, booleanadas ou string.
# Outro formatos de dados, como HDF5, Feather e msgpack, têm os tipos de dados armazenados no formato.

os.getcwd() # Devolve diretório atual
os.chdir("C:/Users/lucas/OneDrive/Documentos") # Change diretório
os.getcwd()
os.listdir() # Lista arquivos do diretório atual
df = pd.read_csv('ex1.txt')
df
df2 = pd.read_table('ex1.txt', sep=','); df2

df3 = pd.read_csv('ex1.txt', header = None); df3
df4 = pd.read_csv('ex1.txt', names = ['a','b','c','d','message']);df4
names = ['a','b','c','d','message']
df5 = pd.read_csv('ex1.txt',names=names,index_col = 'message');df5

parsed = pd.read_csv('csv_mindex.txt',
                     index_col = ['key1','key2'])
parsed

# QUANTIDADE VARIÀVEL DE ESPAÇOS EM BRANCO COMO DELIMITADOR ->>>> pd.read_table com sep = '\s+'
pd.options.display.max_rows = 10
# nrows como argumento de pd.read_csv ajuda a delimitar a quantidade de linhas a serem lidas do arquivo.
# Para ler um arquivo em partes, especifique uma quantidade de linhas para chuncksize
# o objeto TEXTPARSER devolvido por read_csv permite iterar pelas partes do arqujivo de acordo com o chucksize.

# Escrevendo dados em formato-texto (pg.228)

df5.to_csv('out.txt')

# XML e HTML: web scrapping 

# pip install lxml
# pip install beatifulsoup4 html5lib
import lxml
import lxml
import beatifulsoup4
import html5lib

# FORMATO DE DADOS BINÁRIOS ------> Serialização (pickle)

# Uma das formas mais simples de armazenar dados de modo eficiente em formato binário é usando a serialização embutida
#  pickle de python. Todos os objetos do pandas têm um método TO_PICKLE que escreve os dados em disco em formato pickle.

frame = pd.read_csv('ex1.csv')
frame = pd.read_csv('ex1.txt')
frame
frame.to_pickle('frame_pickle')
pd.read_pickle('frame_pickle')

# pickle é recomendado apenas como um formato de armazenagem de curta duração. O problema está no fato de ser difícil de garantir
# que o formato permanecerá estável com o passar do tempo; um objeto serializado com pickle poderá não ser desserializado com uma
# versão mais recente de uma biblioteca. Talvez precisemos romper com o picle em algum momento.

# O pandas tem suporte incluído para outros dois formatros de dados binários: HDF5 e MessagePack. 
# HDF5 é um formato de arqquivo bem visto cujo propósito é armazenar grandes quantidades de dados científicos em arrays.

# Interagindo com Banco de Dados.

# Carregar dados de SQL para um DataFrame é razoavelmente simples, e o pandas tem algumas funções para simplificar o processo.
# Como exemplo, um bando de dados SQLite usando o driver embutido sqlite3 do Python:

import sqlite3
query = """CREATE TABLE test (a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);"""
con = sqlite3.connect('mydata.sqlite')
con.execute(query)
data = [('Atlanta','Georgia',1.25,6),
        ('Tallahassee','Florida',2.6,3),
        ('Sacramento','California',1.7,5)]
stmt = "INSERT INTO test VALUES (?,?,?,?)"
con.executemany(stmt,data)
con.commit()
cursor = con.execute('select * from test')
rows = cursor.fetchall()
rows
cursor.description
pd.DataFrame(rows, columns = [x[0] for x in cursor.description])

import sqlalchemy as sqla
db = sqla.create_engine('sqlite:///mydata.sqlite')
pd.read_sql('select * from test', db)

##########################################################################################


# 7. Limpeza e Preparação dos Dados

# Durante a análise e modelagem dos dados, um período significativo de tempo é gasto em sua preparação: carga, limpeza, transformação
# e reorganização.  Sabe-se que essas tarefas em geral ocupam 80% ou mais do tempo de um analista. às vezes, o modo como os dados
# são armazenados em arquivos ou em bancos de dados não constituem o formato correto para uma tarefa particular. 

# DADOS AUSENTES, DADOS DUPLICADOS, MANIPULACAO DE STRINGS E OUTRAS TRANSFORMACOES DE DADOS

# 7.1 Tratando dados ausentes.

# todas as estatísticas descritivas do pandas excluem automaticamente dados ausentes. 
# df.dropna(axis=1,how='all') só dropa se todos forem igual NaN na coluna
# df.dropna(thresh=2) limita a dropar somente quando NaN for igual a 2 na linha
# Preenchendo dados ausentes ----> df.fillna(0)
# df.fillna({1:0.5,2:0})
# alteração in_place ---> _ = df.fillna(0, inplace = True)

# Removendo duplicatas

data = pd.DataFrame({'k1': ['one','two']*3 + ['two'],
                     'k2': [1,1,2,3,3,4,4]})
data.duplicated()

data = pd.DataFrame({'k1': ['one','two']*3 + ['two'],
                     'k2': [1,1,2,3,3,4,4]})
data.duplicated()
data.drop_duplicates()
data['v1'] = range(7)
data
data.drop_duplicates(['k1'])
data
data.drop_duplicates(['k1','k2'], keep='last')
# duplicated e drop_duplicates, por padrão, mantêm a primeira combnação de valores observados. 
# Passar keep=last devolverá a última.

# Transformando dados usando uma função ou um mapeamento.


data = pd.DataFrame({'k1': ['one','two']*3 + ['two'],
                     'k2': [1,1,2,3,3,4,4]})
data.duplicated()
data.drop_duplicates()
data['v1'] = range(7)
data
data.drop_duplicates(['k1'])
data
data.drop_duplicates(['k1','k2'], keep='last')

# Suponha que quiséssemos adicionar uma coluna informando o tipo do animal do qual cada alimento é proveniente. Vamos criar
# um mapeamento entre cada tipo distinto de carne e o tipo do animal>

meat_to_animal = {
    'bacon': 'pig',
    'pulled pork':'pig',
    'pastrami':'cow',
    'corned beef': 'cow',
    'honey ham': 'pig',
    'nova lox': 'salmon'}

lowercased = data['food'].str.lower(); lowercased
data['animal'] = lowercased.map(meat_to_animal)
data

# Usar map é uma forma conveniente de fazer transformações em todos os elementos e executar outras operações relacionadas
# à limpeza de dados

# Assim como os valores em uma Series, os rótulos dos eixos podem ser transformados de modo semelhante por uma função
# ou alguma forma de mapeamento, a fim de gerar objetos novos com rótulos diferentes. Também podemos modificar os eixos
# in-place, sem criar nova estrutura de dados. 

data = pd.DataFrame(np.arange(12).reshape((3,4)),
                    index = ['Ohio','Colorado','New York'],
                    columns = ['one','two','three','four'])
transform = lambda x: x[:4].upper()
data.index.map(transform)


# COMPARTIMENTALIZAÇÃO - pd.cut
# Objeto criado é Categorical especial. A saída que vemos descreve os bins calculados pelo pandas.cut.
# Podemos tratá-la como um array de strings informando o compartimento, internamente, ela contém um array categories
# que especifica os nomes distintos das categorias junto com rótulos para os dados de ages no atributo codes:

ages = [20,22,25,27,21,23,37,31,61,45,41,32]
bins = [18,25,35,60,100]
cats = pd.cut(ages,bins)
cats
cats.codes
cats.codes
cats.categories
pd.value_counts(cats)
pd.cut(ages,[18,26,36,61,100], right=False)
group_names = ['Youth','YoungAdult','MiddleAged','Senior']
pd.cut(ages,[18,26,36,61,100], right=False, labels = group_names)
# Se passarmos um número inteiro de compartimentos para cut, em vez de passar fronteiras explícitas, ele calculará
# compartimentos de tamanhos iguais com base nos valores mínimo e máximo dos dados.

pd.cut(ages,[18,26,36,61,100], right=False)
group_names = ['Youth','YoungAdult','MiddleAged','Senior']
pd.cut(ages,[18,26,36,61,100], right=False, labels = group_names)

data = np.random.randn(10000)
cats = pd.qcut(data,4)
cats
pd.value_counts(cats)

# DETECTANDO E FILTRANDO VALORES DISCREPANTES
# Filtrar ou transformar valores discrepantes (outliers) é, em boa medida, uma questão de aplicar operações de array.
# Considere um DataFrame com alguns dados normalmente distribuídos.

data = pd.DataFrame(np.random.randn(1000,4))
data.describe()
col = data[2]; col
data
col[np.abs(col) > 3]
data[(np.abs(data)>3).any(1)] # todas as linhas que possuem um valor que exceda -3 ou 3
data = pd.DataFrame(np.random.randn(1000,4))
data.describe()
col = data[2]; col
data
col[np.abs(col) > 3]
data[(np.abs(data)>3).any(1)] # todas as linhas que possuem um valor que exceda -3 ou 3
data[np.abs(data) > 3] = np.sign(data)*3 # retirar valores maiores que -3 ou 3
data.describe()

# Permutação e amostragem aleatória

# Permutação -> numpy.random.permutation
# Para selecionar um subconjunto aleatório sem substituição, o método sample pode ser usado em Series e em DataFrame.

# Calculando varíaveis indicadoras/dummy

# 7.3 - MANIPULAÇÃO DE STRINGS

# facilitada por métodos embutidos (built-in methods) do objeto string. Para uma correspondência de padrões e manipulações de texto mais
# complexas, o uso de expressões regulares talvez seja necessário.

# Uma string separada por vírgulas pode ser dividida em partes usando SPLIT
val = 'a,b,  guido'
val.split(',')

# Com frequência, split é usado em conjunto com strip para remover espaços em branco (incluindo quebras de linha)

pieces = [x.strip() for x in val.split(',')]
pieces
first, second, third = pieces
first + '::' + second + '::' + third
# No entanto, esse não é um método genérico prático. Uma forma mais rápida e pythônica consiste em passar uma lista
# ou uma tupla para o meétodo join na string '::'.

'::'.join(pieces)

'guido' in val
val.index(',')
val
val.find(':')
val.index(':') #lança uma exceção


import re
text = "foo      bar \t baz     \tqux"
re.split('\s+', text)
regex = re.compile('\s+')
regex.split(text)
regex.findall(text)


# CAPITULO 8 - Tratamento de Dados: junção, combinação e reformatação

# Dados espalhados em diversos formatos e fontes. Importante conceito: indexação hierárquica.
# INDEXAÇÃO HIERARQUICA permite ter vários níveis de índice (dois ou mais) em um eixo. De forma até certo ponto,
# abstrata, ela oferece uma maneira de trabalhar com dados de dimensões mais altas em um formato de dimensões menores.

data = pd.Series(np.random.randn(9),
                 index = [['a','a','a','b','b','c','c','d','d'],
                          [1,2,3,1,3,1,2,2,3]])
data.index
data['b']
data.loc[['b','d']]
data.loc[:,2]
data.unstack()
data.unstack().stack()

# Em um DataFrame, qualquer eixo pode ter um índice hierárquico.

# 8.2 - COMBINANDO E MESCLANDO CONJUNTO DE DADOS

# pandas.merge: conecta linhas em DF com base em uma ou mais chaves. Essa operação será conhecida pelos usuários de SQL ou de
# outros bancos relacionais pois ela implementa as operações de junção (join) dos bancos de dados


# pandas.concat -> concatena ou "empilha" objetos ao longo de um eixo

# O método de instância combine_first permite combinar dados que se sobrepõem a fim de preencher valores ausentes em um objeto com valores
# de outros objetos

df1 = pd.DataFrame({'key':['b','b','a','c','a','a','b'],
                    'data1': range(7)})
df2 = pd.DataFrame({'key':['a','b','d'],
                    'data2': range(3)})
df1
df2
pd.merge(df1,df2)
pd.merge(df1,df2, on='key')
df3 = pd.DataFrame({'lkey':['b','b','a','c','a','a','b'],
                    'data1': range(7)})
df4 = pd.DataFrame({'rkey':['a','b','d'],
                    'data2': range(3)})

pd.merge(df3,df4, left_on='lkey', right_on='rkey')

# Por padrão, merge executa uma junção do tipo 'inner'. as chaves no resultado sao a interseccão ou conjunto comum
# que se encontra as duas tabelas. Outras opções são left, right, outer.
# A junção externa (outer join) efetua união das chaves, combinando o efeito da aplicação das junções tanto à esquerda quanto
# à direita
pd.merge(df1,df2,how='outer') # vai ter valor NaN
pd.merge(df1,df2,how='outer')
pd.merge(df1,df2,how='left')
pd.merge(df1,df2,how='right')

# REGORGANIZANDO E ORDENANDO NÍVEIS: Ocasionalmente precisaremos reorganizar a ordem dos níveis em um eixo
# ou ordenar os dados de acordo com os valores em um nível específico. swaplevel aceita dois números
# ou nomes de níveis e devolve um novo objeto com níveis trocados.
import pandas as pd
import numpy as np
frame = pd.DataFrame(np.arange(12).reshape(4,3),
                     index = [['a','a','b','b'],[1,2,1,2]],
                     columns = [['Ohio','Ohio','Colorado'],['Green','Red','Green']])
frame.index.names = ['key1','key2']
frame.columns.names = ['state','color']
frame

#  Tome cuidado para distinguir os nomes dos índices 'state' e 'color' dos rótulos das linhas.
# Com uma indexação parcial de colunas, você poderá, de modo semelhante, selecionar grupos de colunas:

frame['Ohio']
frame.swaplevel('key1','key2')

# sort_index ordena os dados usando os valores de um só nível. 

# ESTATÌSTICAS DE RESUMO POR NÍVEL: muitas estatísticas descriticas ou de resumo em DataFrame e em Series têm uma opção level;
# com ela, podemos especificar o nível de acordo com o qual queremos fazer uma agregação, em um eixo em particular.
# Considere o DataFrame anterior; podemos fazer uma agregação por nível, seja nas linhas ou nas colunas, da seguinte maneira>
frame.sum(level='key2')
frame.sum(level='color',axis=1)
frame

# INDEXANDO COM AS COLUNAS DE UM DATAFRAME: não é incomum querer usar ou uma mais colunas de um DataFrame como índice de linha;
# de modo alternativo, talvez você queira mover o índice das linhas para as colunas do DataFrame.

frame = pd.DataFrame({'a':range(7),
                      'b':range(7,0,-1),
                      'c':['one','one','one','two','two','two','two'],
                      'd':[0,1,2,0,1,2,3]})
                      
# a função set_index de DataFrame criará um novo DataFrame usando uma ou mais de suas colunas como índice
 
frame2 = frame.set_index(['c','d'])
frame2
frame2 = frame.set_index(['c','d'], drop = False)
frame2

# pandas.merge: conecta linhas em DataFrames com base em uma ou mais chaves. Essa operação será conhecida pelos usuários de 
# SQL e outros bancos de dados relacionais, pois ela implementa as operações de junção (join) dos bancos de dados.
# pandas.concat -> concatena ou " empilha " objetos ao longo de um eixo.
# O método de instância combine_first permite combinar dados que se sobrepõem a fim de preencher valores ausentes em um objeto
# com valores de outro objeto

arr = np.arange(12).reshape(3,4)
stack_columns = np.concatenate([arr, arr],axis=1)
stack_rows = np.concatenate([arr, arr],axis=0)

s1 = pd.Series([0,1], index = ['a','b'])
s2 = pd.Series([2,3,4], index = ['c','d','e'])
s3 = pd.Series([5,6],index=['f','g'])
pd.concat([s1,s2,s3])

# Por padrão, pd.concat atua em axis = 0. Gerando outra pd.Series. 


arr = np.arange(12).reshape(3,4); arr
stack_columns = np.concatenate([arr, arr],axis=1)
stack_rows = np.concatenate([arr, arr],axis=0)
s1 = pd.Series([0,1], index = ['a','b'])
s2 = pd.Series([2,3,4], index = ['c','d','e'])
s3 = pd.Series([5,6],index=['f','g'])
pd.concat([s1,s2,s3]) # Gera outra series
pd.concat([s1,s2,s3], axis = 1) # Gera um dataframe

s4 = pd.concat([s1,s3])
pd.concat([s1,s4], axis=1)
pd.concat([s1,s4], axis =1, join='inner')

# REFORMATACAO E PIVOTEAMENTO

# Reformatação com indexação hierárquica. A indexação hierárquica oferece uma forma consistente de reorganizar os dados
# em um DataFrame. Há duas ações principais
##################### STACK --> faz a "rotação" ou o pivoteamento das colunas dos dados para as linhas
##################### UNSTACK --> faz o pivoteamento das linhas para as colunas

data = pd.DataFrame(np.arange(6).reshape((2,3)),
                    index = pd.Index(['Ohio','Colorado'], name = 'state'),
                    columns = pd.Index(['one','two','three'],name = 'number'))
result = data.stack()
result # faz o pivoteamento das colunas para as linhas
result.unstack()
result.unstack(0)
result.unstack('state')

# CAPITULO 10 - Agregação de Dados e Operações em Grupos

# Classificar um conjunto de dados e aplicar uma função a cada grupo, seja uma agregação ou uma transformação,
# com frequência é um componente essencial em um fluxo de trabalho de análise de dados. Após carregar, mesclar e preparar
# um conjunto de dados, talvez seja necessário calcular estatísticas de grupos ou, possivelmente, tabelas pivôs visando a 
# relatórios e visualizações. O pandas oferece uma interface flexível groupby, que permite manipular e resumir conjuntos
# de dados de forma natural.

##################### Separar um objeto do pandas em partes usando uma ou mais chaves (na forma de funções, arrays ou nomes de colunas de um DF

##################### calcular estatísticas de resumo para grupos, como contador, média ou desvio-padrão, ou aplicar função definida pelo usuário

##################### aplicar transformações em grupo ou fazer outras manipulações como normalização, regressão linear, classificação ou seleção de subconjuntos

##################### calcular tabelas pivôs e tabulações cruzadas

##################### fazer análises de quantis e outras análises estatítsticas de grupos


# 10.1 Funcionamento de Groupby


df = pd.DataFrame({'key1' : ['a','a','b','b','a'],
                  'key2' : ['one','two','one','two','one'],
                  'data1': np.random.randn(5),
                  'data2': np.random.randn(5)})

grouped = df['data1'].groupby(df['key1'])
grouped.mean()
means = df['data1'].groupby([df['key1'], df['key2']]).mean()
means
means.unstack()

df.groupby('key1').mean()
df.groupby(['key1','key2']).mean()

# O objeto GroupBy aceita iteração, gerando uma sequência de tuplas de 2 contendo
# o nome do grupo, junto com a porção de dados. 

for name, group in df.groupby('key1'):
    print(name)
    print(group)

for (k1,k2),group in df.groupby(['key1','key2']):
    print((k1,k2))
    print(group)
    
# Por padrão, groupby agrupa em axis = 0, mas podemos agrupar em qualquer um dos outros eixos.

# Selecionando uma coluna ou um subconjunto de colunas

# Indexar um objeto GroupBy criado a partir de um DataFrame com um nome de coluna ou um array de nomes de coluna tem o efeito de criar 
# subconjuntos de colunas para agregação. Isso significa que:

people = pd.DataFrame(np.random.randn(5,5),
                      columns = ['a','b','c','d','e'],
                      index = ['Joe','Steve','Wes','Jim','Travis'])

people.iloc[2:3,[1,2]] = np.nan
mapping = {'a': 'red',
           'b': 'red',
           'c': 'blue',
           'd': 'blue',
           'e': 'red',
           'f': 'orange'}
by_column = people.groupby(mapping, axis=1)
by_column.sum()
map_series = pd.Series(mapping)
map_series
people.groupby(map_series,axis=1).count()

# AGRUPANDO COM FUNÇÔES

# Usar funções Python é uma forma mais genérica de definir um mapeamento de grupos, em comparação com um dicionário ou uma Series.
# Qualquer função passada como uma chave de grupo será chamada uma vez por valor de índice, com os valores de retorno usados
# como os nomes dos grupos.

people.groupby(len).sum()
key_list = ['one','one','one','two','two']
people.groupby([len, key_list]).min()
columns = pd.MultiIndex.from_arrays([['US','US','US','JP','JP'],
                                     [1,3,5,1,3]],
                                    names = ['cty','tenor'])
hier_df = pd.DataFrame(np.random.randn(4,5), columns = columns)
hier_df
hier_df.groupby(level='cty',axis=1).count()

# MÈTODO APPLY: separar-aplicar-combinar genérico: o método de propósito mais geral de GroupBy é apply.
# apply serpara o objeto sendo manipulado em partes, chama a função recebida em parte e, em seguida, tenta concatená-las.
# apply só precisar devolver um objeto do pandas ou valor escalar. O que ocorre dentro da função passada para esse método é 
# de sua responsabilidade.

# Exemplo: preenchendo valores ausentes com valores específicos de grupo.
# Ao limpar dados ausentes, em alguns casos, você substituirá os dados observados por dropna, porém, em outros, talvez você 
# queira representar os valores nulos (NA), isto é, preenchê-los, com um valor fixo ou outro valor derivado de dados. 
# fillna é a ferramenta correta a ser usada;

s = pd.Series(np.random.randn(6))
s[::2] = np.nan
s.fillna(s.mean())

# Suponha que seja necessário que o valor de preenchimento varie conforme
# o grupo. Um modo de fazer isso é agrupar os dados e usar apply com uma função
# que chame fillna em cada porção de dados. 

states = ['Ohio','New York', 'Vermont', 'Florida', 'Oregon', 'Nevada',
          'California', 'Idaho']

group_key = ['East'] * 4 + ['West']*4
data = pd.Series(np.random.randn(8), index=states)
data
data[['Vermont','Nevada','Idaho']] = np.nan; data
data.groupby(group_key).mean()
fill_mean = lambda g: g.fillna(g.mean())
data.groupby(group_key).apply(fill_mean)

fill_values = {'East' : 0.5, 'West': -1}
fill_func = lambda g: g.fillna(fill_values[g.name])
data.groupby(group_key).apply(fill_func)

# EXEMPLO: amostragem aleatória e permutação

# Suponha que quiséssemos sortear uma amostra aleatória (com ou sem substituição)
# a partir de um conjunto de dados grande, visando a uma simulação de Monte Carlo
# ou outra aplicação.

suits = ['H','S','C','D']
card_val = (list(range(1,11)) + [10]*3)*4
base_names = ['A'] + list(range(2,11)) + ['J','K','Q']
cards = []
for suit in ['H','S','C','D']:
    cards.extend(str(num) + suit for num in base_names)
deck = pd.Series(card_val, index=cards)

def draw(deck, n=5):
    return deck.sample(n)

draw(deck)
# Suponha que quiséssemos duas cartas aleatórias de cada naipe. Como o naipe é o último
# caractere do nome de cada carta, podemos fazer agrupamentos com base nisso e usar apply:

get_suit = lambda card: card[-1]
deck.groupby(get_suit).apply(draw,n=2)

# de modo alternativo, poderíamos escrever:

deck.groupby(get_suit, group_keys=False).apply(draw, n=2)

# EXEMPLO: média ponderada de grupos e correlação

# No paradigma separar-aplicar-combinar (split-array-combine) de groupby, operações entre clunas em um DataFrame ou em duas Series,
# como uma média ponderada de grupos, são possíveis. 

df = pd.DataFrame({'category': ['a','a','a','a',
                                'b','b','b','b'],
                   'data' : np.random.randn(8),
                   'weights': np.random.randn(8)})

df
grouped = df.groupby('category')
get_wavg = lambda g: np.average(g['data'], weights=g['weights'])
grouped.apply(get_wavg)

# 10.4 - TABELAS PIVÔS e TABULAÇÃO CRUZADA

# Uma tabela PIVÔ é uma ferramenta de SINTETIZAÇÃO DE DADOS frequentemente encontrada
# em programas de planilhas e em outros softwares de análise de dados. 
# Ela agrega uma tabela de dados de acordo com uma ou mais chaves, organizando os dados
# em um retângulo com algumas das chaves dos grupos nas linhas e outras nas colunas.
# As tabelas pivô em Python com pandas são possíveis por meio do recurso groupby,
# em conjunto com operações de reformatação que utilizam indexação hierárquica. 
# O DataFrame tem um método pivot_table e há também uma função pandas.pivot_tabçe de nível superior

# Além de oferecer uma interface conveniente para groupby, pivot_table pode somar totais
# parciais, também conhecidos como margens (margins):












